ğŸš€ Project Setup & Documentation

1. ğŸ“¦ Install Dependencies
pip install -r requirements.txt

2. â–¶ï¸ Run the Program (main.py)
python main.py

3. ğŸŒ Launch the Web Interface
uvicorn web.app:app --reload --port 8000
Then open your browser and go to ğŸ‘‰ http://127.0.0.1:8000

4. ğŸ” Overview of PhoBERT Model

PhoBERT for Sentiment Analysis

PhoBERT is a pre-trained language model for Vietnamese, based on the RoBERTa architecture, trained on large-scale Vietnamese text corpora.

It is highly effective for several NLP tasks, including:

Sentiment Analysis

Text Classification

Part-of-Speech Tagging

Other Natural Language Processing applications

ğŸ“– Documentation: https://huggingface.co/vinai/phobert-base


5. ğŸ¨ VAE Seq2Seq Model (Variational AutoEncoder - Sequence to Sequence)
Variational Autoencoder (VAE) combined with Sequence-to-Sequence (Seq2Seq) is applied for sequence generation tasks (e.g., text generation).

VAE: Encodes input into a latent space, enabling the generation of new, diverse data that follows the original distribution.

Seq2Seq: Encodes an input sequence into a hidden vector (encoder) and reconstructs or generates an output sequence (decoder).

VAE + Seq2Seq: Together, they allow the model to both learn latent representations and generate more creative, diverse sequences.

Reference Paper: https://arxiv.org/abs/1511.06349

1. ğŸ“¦ CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n

pip install -r requirements.txt

2. â–¶ï¸ Cháº¡y chÆ°Æ¡ng trÃ¬nh (file main.py)

\*\* Bá» comment á»Ÿ bÆ°á»›c 5 (Ä‘á»ƒ táº¡o sentiment)
python main.py

3. ğŸŒ Hiá»ƒn thá»‹ giao diá»‡n web

uvicorn web.app:app --reload --port 8000
truy cáº­p trang web: http://127.0.0.1:8000

4. ğŸ” Tá»•ng quan vá» model PhoBERT
PhoBERT cho Sentiment Analysis
PhoBERT lÃ  má»™t pre-trained language model cho tiáº¿ng Viá»‡t, Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn kiáº¿n trÃºc RoBERTa vÃ  huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u tiáº¿ng Viá»‡t lá»›n. 
Model nÃ y ráº¥t máº¡nh cho cÃ¡c tÃ¡c vá»¥ NLP nhÆ° phÃ¢n loáº¡i cáº£m xÃºc (sentiment analysis), 
phÃ¢n loáº¡i vÄƒn báº£n, gÃ¡n nhÃ£n tá»« loáº¡i, vÃ  nhiá»u á»©ng dá»¥ng khÃ¡c trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn.

Link tÃ i liá»‡u: https://huggingface.co/vinai/phobert-base

5. ğŸ¨ Model VAE Seq2Seq (Variational AutoEncoder - Sequence to Sequence)
Variational Autoencoder (VAE) káº¿t há»£p vá»›i kiáº¿n trÃºc Sequence-to-Sequence (Seq2Seq) Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ sinh chuá»—i dá»¯ liá»‡u (vÃ­ dá»¥: sinh vÄƒn báº£n).
VAE giÃºp mÃ´ hÃ¬nh hÃ³a dá»¯ liá»‡u Ä‘áº§u vÃ o trong má»™t latent space (khÃ´ng gian tiá»m áº©n), tá»« Ä‘Ã³ cÃ³ thá»ƒ sinh ra dá»¯ liá»‡u má»›i cÃ³ tÃ­nh Ä‘a dáº¡ng vÃ  gáº§n vá»›i phÃ¢n phá»‘i gá»‘c.
Seq2Seq cung cáº¥p kháº£ nÄƒng mÃ£ hÃ³a má»™t chuá»—i Ä‘áº§u vÃ o thÃ nh vector áº©n (encoder), sau Ä‘Ã³ giáº£i mÃ£ thÃ nh chuá»—i Ä‘áº§u ra (decoder). 
Khi káº¿t há»£p vá»›i VAE, mÃ´ hÃ¬nh cÃ³ thá»ƒ vá»«a há»c Ä‘Æ°á»£c biá»ƒu diá»…n tiá»m áº©n, vá»«a sinh ra cÃ¡c chuá»—i má»›i má»™t cÃ¡ch sÃ¡ng táº¡o.
