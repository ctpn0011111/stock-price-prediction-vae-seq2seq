import os
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime

input_dir = "../../dataset/link/pvg/"
output_dir = "../../dataset/data/pvg/"
os.makedirs(output_dir, exist_ok=True)

output_file = os.path.join(output_dir, "articles.json")


def crawl_page(url):
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
    except Exception as e:
        print(f"âŒ Lá»—i khi crawl {url}: {e}")
        return None

    soup = BeautifulSoup(r.text, "html.parser")

    # --- Láº¥y title ---
    title_tag = soup.select_one("div.dnw-title a h1")
    title = title_tag.get_text(strip=True) if title_tag else ""

    # --- Láº¥y ngÃ y Ä‘Äƒng ---
    pub_date_tag = soup.select_one("div.dnw-time a span")
    pub_date = pub_date_tag.get_text(strip=True) if pub_date_tag else ""

    # --- Láº¥y content ---
    content_parts = []

    # Láº¥y h3.desc
    desc_tag = soup.select_one("div.dnw-center h3.desc")
    if desc_tag:
        content_parts.append(desc_tag.get_text(strip=True))

    # Láº¥y táº¥t cáº£ p con trá»±c tiáº¿p cá»§a div.dnw-center
    for p in soup.select("div.dnw-center > p"):
        content_parts.append(p.get_text(" ", strip=True))

    # Láº¥y text trong table td (ká»ƒ cáº£ td > p)
    for td in soup.select("div.dnw-center table td"):
        text = td.get_text(" ", strip=True)
        if text:
            content_parts.append(text)

    content = "\n".join(content_parts).strip()

    return title, pub_date, content


def main():
    results = []

    # Duyá»‡t qua táº¥t cáº£ file JSON trong thÆ° má»¥c input_dir
    for file_name in os.listdir(input_dir):
        if not file_name.endswith(".json"):
            continue
        input_file = os.path.join(input_dir, file_name)
        print(f"ğŸ“‚ Äang xá»­ lÃ½ file: {input_file}")

        with open(input_file, "r", encoding="utf-8") as f:
            data = json.load(f)

        for item in data:
            url = item.get("url")
            print(f"ğŸ” Crawling {url} ...")
            crawled = crawl_page(url)
            if not crawled:
                continue

            title, pub_date, content = crawled
            results.append(
                {
                    "id": item.get("id"),
                    "title": title,
                    "url": url,
                    "content": content,
                    "pub_date": pub_date,
                    "crawl_date": item.get(
                        "crawl_date", datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    ),
                }
            )

    # Xuáº¥t toÃ n bá»™ káº¿t quáº£ ra má»™t file duy nháº¥t
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"âœ… ÄÃ£ lÆ°u {len(results)} bÃ i viáº¿t vÃ o {output_file}")


if __name__ == "__main__":
    main()
