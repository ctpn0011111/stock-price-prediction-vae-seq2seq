import os
import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import urllib3

# Táº¯t cáº£nh bÃ¡o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

BASE_URL = "https://www.pvtrans.com"

# ğŸ“‚ ÄÆ°á»ng dáº«n thÆ° má»¥c lÆ°u dá»¯ liá»‡u
SAVE_DIR = "../../dataset/data/bsr"
os.makedirs(SAVE_DIR, exist_ok=True)
SAVE_PATH = os.path.join(SAVE_DIR, "output.json")

# ğŸ“‚ ÄÆ°á»ng dáº«n file JSON chá»©a URL
INPUT_PATH = "../../dataset/link/bsr/pvtrans_data.json"

# ğŸ“„ Äá»c file JSON Ä‘áº§u vÃ o
with open(INPUT_PATH, "r", encoding="utf-8") as f:
    data = json.load(f)

output = []

for idx, item in enumerate(data, start=1):
    full_url = BASE_URL + item["url"]

    resp = requests.get(full_url, verify=False, timeout=15)
    soup = BeautifulSoup(resp.text, "html.parser")

    # ---- Láº¥y tiÃªu Ä‘á» ----
    title_tag = soup.select_one("div.container h4#blog_post_name")
    title = title_tag.get_text(strip=True) if title_tag else item.get("title", "")

    # ---- Láº¥y ngÃ y Ä‘Äƒng ----
    date_tag = soup.select_one("div.entry-meta time.entry-date")
    ngay_dang = date_tag.get_text(strip=True) if date_tag else ""

    # ---- Láº¥y ná»™i dung ----
    content_parts = []

    # Pháº§n 1: ná»™i dung trong h5.entry-title > p
    h5_tag = soup.select_one("h5.entry-title p")
    if h5_tag:
        content_parts.append(h5_tag.get_text(" ", strip=True))

    # Pháº§n 2: ná»™i dung cÃ¡c span trong div.blog_content div.col-md-12.mb16.mt16
    spans = soup.select("div.blog_content div.col-md-12.mb16.mt16 span")
    for sp in spans:
        text = sp.get_text(" ", strip=True)
        if text:
            content_parts.append(text)

    content = "\n".join(content_parts)

    # ---- NgÃ y crawl ----
    ngay_crawl = datetime.now().strftime("%Y-%m-%d")

    # ---- Ghi dá»¯ liá»‡u ----
    output.append(
        {
            "id": idx,
            "title": title,
            "url": full_url,
            "content": content,
            "ngay_dang": ngay_dang,
            "ngay_crawl": ngay_crawl,
        }
    )

# ğŸ’¾ LÆ°u ra file JSON
with open(SAVE_PATH, "w", encoding="utf-8") as f:
    json.dump(output, f, ensure_ascii=False, indent=4)

print(f"âœ… ÄÃ£ crawl xong, lÆ°u táº¡i: {SAVE_PATH}")
