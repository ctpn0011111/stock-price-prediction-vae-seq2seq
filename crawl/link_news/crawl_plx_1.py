import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime


class PetrolimexPressCrawler:
    def __init__(self, start_page=1, end_page=1, output_dir="../dataset/link/plx"):
        self.base_url = "https://www.petrolimex.com.vn/ndi/thong-cao-bao-chi/"
        self.start_page = start_page
        self.end_page = end_page
        self.output_dir = output_dir
        self.create_output_directory()

    def create_output_directory(self):
        """Táº¡o thÆ° má»¥c lÆ°u dá»¯ liá»‡u náº¿u chÆ°a cÃ³"""
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

    def crawl_page(self, page_num):
        """Crawl dá»¯ liá»‡u tá»« má»™t trang"""
        url = f"{self.base_url}{page_num}.html"
        response = requests.get(url)

        if response.status_code != 200:
            print(f"âŒ KhÃ´ng táº£i Ä‘Æ°á»£c trang {page_num} (status {response.status_code})")
            return []

        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        crawl_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # TÃ¬m táº¥t cáº£ div chá»©a ná»™i dung
        bodies = soup.find_all("div", class_="post-default__body")

        for idx, body in enumerate(bodies, 1):
            h3 = body.find("h3", class_="post-default__title")
            if not h3:
                continue

            a_tag = h3.find("a")
            if not a_tag:
                continue

            title = a_tag.get_text(strip=True)
            href = a_tag.get("href", "")

            # Xá»­ lÃ½ URL tuyá»‡t Ä‘á»‘i
            if href.startswith("/"):
                href = "https://www.petrolimex.com.vn" + href

            article = {
                "id": f"page_{page_num}_{idx}",
                "title": title,
                "url": href,
                "crawl_date": crawl_date,
            }
            articles.append(article)

        print(f"âœ… Trang {page_num}: láº¥y Ä‘Æ°á»£c {len(articles)} bÃ i viáº¿t")
        return articles

    def save_page_data(self, articles, page_num):
        """LÆ°u dá»¯ liá»‡u má»—i trang thÃ nh file JSON"""
        if not articles:
            return

        filename = f"plx_press_page_{page_num}.json"
        filepath = os.path.join(self.output_dir, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ ÄÃ£ lÆ°u {len(articles)} bÃ i viáº¿t vÃ o {filepath}")

    def crawl(self):
        """Crawl tá»« start_page Ä‘áº¿n end_page"""
        for page in range(self.start_page, self.end_page + 1):
            articles = self.crawl_page(page)
            self.save_page_data(articles, page)


def main():
    # âœ… Chá»n sá»‘ trang cáº§n crawl
    start_page = 1
    end_page = 25

    crawler = PetrolimexPressCrawler(start_page=start_page, end_page=end_page)
    crawler.crawl()


if __name__ == "__main__":
    main()
