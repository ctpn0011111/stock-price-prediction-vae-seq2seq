import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import json
import os
from datetime import datetime
import time
import logging


class HPTCrawler:
    def __init__(self):
        self.base_url = "https://hpt.vn/newscate/27"
        self.output_dir = "../dataset/link/hpt"
        self.setup_logging()
        self.setup_driver()
        self.create_output_directory()

    def setup_logging(self):
        """Thi·∫øt l·∫≠p logging ƒë·ªÉ theo d√µi qu√° tr√¨nh crawl"""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[logging.FileHandler("hpt_crawler.log"), logging.StreamHandler()],
        )
        self.logger = logging.getLogger(__name__)

    def setup_driver(self):
        """Thi·∫øt l·∫≠p Selenium WebDriver"""
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # Ch·∫°y ·ªü ch·∫ø ƒë·ªô headless
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument(
            "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )

        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.logger.info("WebDriver ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng")
        except Exception as e:
            self.logger.error(f"L·ªói kh·ªüi t·∫°o WebDriver: {e}")
            raise

    def create_output_directory(self):
        """T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ n·∫øu ch∆∞a t·ªìn t·∫°i"""
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
            self.logger.info(f"ƒê√£ t·∫°o th∆∞ m·ª•c: {self.output_dir}")

    def extract_data_from_current_page(self, page_num):
        """Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ trang hi·ªán t·∫°i"""
        articles = []

        try:
            # ƒê·ª£i trang load ho√†n to√†n
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.content"))
            )

            # L·∫•y HTML source v√† parse b·∫±ng BeautifulSoup
            soup = BeautifulSoup(self.driver.page_source, "html.parser")

            # T√¨m t·∫•t c·∫£ div v·ªõi class 'content'
            content_divs = soup.find_all("div", class_="content")

            for content_div in content_divs:
                try:
                    # T√¨m th·∫ª a trong content div
                    a_tag = content_div.find("a")

                    if a_tag:
                        href = a_tag.get("href", "")

                        # T√¨m th·∫ª h1 trong th·∫ª a
                        h1_tag = a_tag.find("h1")

                        if h1_tag:
                            title = h1_tag.get_text(strip=True)

                            # X·ª≠ l√Ω URL t∆∞∆°ng ƒë·ªëi
                            if href.startswith("/"):
                                full_url = f"https://hpt.vn{href}"
                            elif not href.startswith("http"):
                                full_url = f"https://hpt.vn/{href}"
                            else:
                                full_url = href

                            article = {"title": title, "url": full_url}
                            articles.append(article)

                except Exception as e:
                    self.logger.warning(
                        f"L·ªói khi x·ª≠ l√Ω m·ªôt b√†i vi·∫øt tr√™n trang {page_num}: {e}"
                    )
                    continue

            self.logger.info(
                f"ƒê√£ tr√≠ch xu·∫•t {len(articles)} b√†i vi·∫øt t·ª´ trang {page_num}"
            )
            return articles

        except Exception as e:
            self.logger.error(f"L·ªói khi tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ trang {page_num}: {e}")
            return []

    def save_page_data(self, articles, page_num):
        """L∆∞u d·ªØ li·ªáu c·ªßa m·ªôt trang v√†o file JSON"""
        if not articles:
            return

        filename = f"hpt_page_{page_num}.json"
        filepath = os.path.join(self.output_dir, filename)

        # Th√™m ID t·ª± ƒë·ªông tƒÉng v√† ng√†y crawl
        crawl_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        formatted_articles = []
        for idx, article in enumerate(articles, 1):
            formatted_article = {
                "id": f"page_{page_num}_{idx}",
                "title": article["title"],
                "url": article["url"],
                "crawl_date": crawl_date,
            }
            formatted_articles.append(formatted_article)

        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(formatted_articles, f, ensure_ascii=False, indent=2)

            self.logger.info(
                f"ƒê√£ l∆∞u {len(formatted_articles)} b√†i vi·∫øt v√†o {filepath}"
            )

        except Exception as e:
            self.logger.error(f"L·ªói khi l∆∞u file {filepath}: {e}")

    def save_all_data(self, all_articles):
        """L∆∞u t·∫•t c·∫£ d·ªØ li·ªáu v√†o m·ªôt file t·ªïng h·ª£p"""
        if not all_articles:
            return

        filename = "hpt_all_articles.json"
        filepath = os.path.join(self.output_dir, filename)

        # Th√™m ID t·ª± ƒë·ªông tƒÉng v√† ng√†y crawl
        crawl_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        formatted_articles = []
        for idx, article in enumerate(all_articles, 1):
            formatted_article = {
                "id": idx,
                "title": article["title"],
                "url": article["url"],
                "crawl_date": crawl_date,
            }
            formatted_articles.append(formatted_article)

        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(formatted_articles, f, ensure_ascii=False, indent=2)

            self.logger.info(
                f"ƒê√£ l∆∞u t·ªïng c·ªông {len(formatted_articles)} b√†i vi·∫øt v√†o {filepath}"
            )

        except Exception as e:
            self.logger.error(f"L·ªói khi l∆∞u file t·ªïng h·ª£p {filepath}: {e}")

    def click_next_page(self):
        """Click v√†o n√∫t next ƒë·ªÉ chuy·ªÉn trang"""
        try:
            # T√¨m th·∫ª a c√≥ class 'next fa fa-caret-right'
            next_button = WebDriverWait(self.driver, 10).until(
                EC.element_to_be_clickable(
                    (By.CSS_SELECTOR, "a.next.fa.fa-caret-right")
                )
            )

            # Click v√†o n√∫t next
            self.driver.execute_script("arguments[0].click();", next_button)
            self.logger.info("ƒê√£ click v√†o n√∫t Next ƒë·ªÉ chuy·ªÉn trang")

            # ƒê·ª£i trang load
            time.sleep(3)
            return True

        except TimeoutException:
            self.logger.info("Kh√¥ng t√¨m th·∫•y n√∫t Next - c√≥ th·ªÉ ƒë√£ ƒë·∫øn trang cu·ªëi")
            return False
        except Exception as e:
            self.logger.error(f"L·ªói khi click n√∫t Next: {e}")
            return False

    def crawl_all_pages(self):
        """Crawl t·∫•t c·∫£ c√°c trang b·∫±ng c√°ch click Next"""
        page_num = 1
        all_articles = []

        self.logger.info("B·∫Øt ƒë·∫ßu crawl d·ªØ li·ªáu t·ª´ HPT")

        try:
            # Truy c·∫≠p trang ƒë·∫ßu ti√™n
            self.driver.get(self.base_url)
            self.logger.info(f"ƒê√£ truy c·∫≠p trang ƒë·∫ßu ti√™n: {self.base_url}")

            while True:
                self.logger.info(f"ƒêang crawl trang {page_num}...")

                # Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ trang hi·ªán t·∫°i
                articles = self.extract_data_from_current_page(page_num)

                if articles:
                    # L∆∞u d·ªØ li·ªáu trang hi·ªán t·∫°i
                    self.save_page_data(articles, page_num)

                    # Th√™m v√†o danh s√°ch t·ªïng
                    all_articles.extend(articles)

                    self.logger.info(
                        f"ƒê√£ ho√†n th√†nh trang {page_num} v·ªõi {len(articles)} b√†i vi·∫øt"
                    )
                else:
                    self.logger.warning(f"Trang {page_num} kh√¥ng c√≥ d·ªØ li·ªáu")

                # Th·ª≠ click Next ƒë·ªÉ chuy·ªÉn sang trang ti·∫øp theo
                if not self.click_next_page():
                    self.logger.info("ƒê√£ crawl h·∫øt t·∫•t c·∫£ c√°c trang")
                    break

                # TƒÉng s·ªë trang
                page_num += 1

                # Delay ƒë·ªÉ tr√°nh b·ªã block
                time.sleep(2)

            # L∆∞u file t·ªïng h·ª£p
            if all_articles:
                self.save_all_data(all_articles)
                self.logger.info(
                    f"Ho√†n th√†nh crawl! T·ªïng c·ªông {len(all_articles)} b√†i vi·∫øt t·ª´ {page_num} trang"
                )
            else:
                self.logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c crawl")

        except Exception as e:
            self.logger.error(f"L·ªói trong qu√° tr√¨nh crawl: {e}")

    def crawl_pages_range(self, max_pages):
        """Crawl m·ªôt s·ªë trang gi·ªõi h·∫°n"""
        page_num = 1
        all_articles = []

        self.logger.info(f"B·∫Øt ƒë·∫ßu crawl t·ªëi ƒëa {max_pages} trang t·ª´ HPT")

        try:
            # Truy c·∫≠p trang ƒë·∫ßu ti√™n
            self.driver.get(self.base_url)
            self.logger.info(f"ƒê√£ truy c·∫≠p trang ƒë·∫ßu ti√™n: {self.base_url}")

            while page_num <= max_pages:
                self.logger.info(f"ƒêang crawl trang {page_num}/{max_pages}...")

                # Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ trang hi·ªán t·∫°i
                articles = self.extract_data_from_current_page(page_num)

                if articles:
                    # L∆∞u d·ªØ li·ªáu trang hi·ªán t·∫°i
                    self.save_page_data(articles, page_num)

                    # Th√™m v√†o danh s√°ch t·ªïng
                    all_articles.extend(articles)

                    self.logger.info(
                        f"ƒê√£ ho√†n th√†nh trang {page_num} v·ªõi {len(articles)} b√†i vi·∫øt"
                    )
                else:
                    self.logger.warning(f"Trang {page_num} kh√¥ng c√≥ d·ªØ li·ªáu")

                # N·∫øu ch∆∞a ƒë·∫øn trang cu·ªëi c√πng, th·ª≠ click Next
                if page_num < max_pages:
                    if not self.click_next_page():
                        self.logger.info(
                            f"Kh√¥ng th·ªÉ chuy·ªÉn sang trang ti·∫øp theo - d·ª´ng ·ªü trang {page_num}"
                        )
                        break

                # TƒÉng s·ªë trang
                page_num += 1

                # Delay ƒë·ªÉ tr√°nh b·ªã block
                time.sleep(2)

            # L∆∞u file t·ªïng h·ª£p
            if all_articles:
                self.save_all_data(all_articles)
                self.logger.info(
                    f"Ho√†n th√†nh crawl! T·ªïng c·ªông {len(all_articles)} b√†i vi·∫øt t·ª´ {page_num-1} trang"
                )
            else:
                self.logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c crawl")

        except Exception as e:
            self.logger.error(f"L·ªói trong qu√° tr√¨nh crawl: {e}")

    def close(self):
        """ƒê√≥ng WebDriver"""
        if hasattr(self, "driver"):
            self.driver.quit()
            self.logger.info("ƒê√£ ƒë√≥ng WebDriver")


def get_user_input():
    """L·∫•y input t·ª´ ng∆∞·ªùi d√πng ƒë·ªÉ ch·ªçn ch·∫ø ƒë·ªô crawl"""
    print("=== HPT Web Crawler ===")
    print("Ch·ªçn ch·∫ø ƒë·ªô crawl:")
    print("1. Crawl t·∫•t c·∫£ trang (click Next ƒë·∫øn h·∫øt)")
    print("2. Crawl s·ªë trang gi·ªõi h·∫°n")

    while True:
        try:
            choice = int(input("\nNh·∫≠p l·ª±a ch·ªçn (1 ho·∫∑c 2): "))
            if choice in [1, 2]:
                return choice
            else:
                print("Vui l√≤ng nh·∫≠p 1 ho·∫∑c 2")
        except ValueError:
            print("Vui l√≤ng nh·∫≠p s·ªë h·ª£p l·ªá")


def get_max_pages():
    """L·∫•y s·ªë trang t·ªëi ƒëa t·ª´ ng∆∞·ªùi d√πng"""
    while True:
        try:
            max_pages = int(input("Nh·∫≠p s·ªë trang t·ªëi ƒëa mu·ªën crawl: "))
            if max_pages > 0:
                return max_pages
            else:
                print("S·ªë trang ph·∫£i > 0")
        except ValueError:
            print("Vui l√≤ng nh·∫≠p s·ªë h·ª£p l·ªá")


def main():
    crawler = HPTCrawler()

    try:
        choice = get_user_input()

        if choice == 1:
            print("\nüöÄ B·∫Øt ƒë·∫ßu crawl t·∫•t c·∫£ trang...")
            crawler.crawl_all_pages()
        else:
            max_pages = get_max_pages()
            print(f"\nüöÄ B·∫Øt ƒë·∫ßu crawl t·ªëi ƒëa {max_pages} trang...")
            crawler.crawl_pages_range(max_pages)

    except KeyboardInterrupt:
        crawler.logger.info("Crawl b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
    except Exception as e:
        crawler.logger.error(f"L·ªói kh√¥ng mong ƒë·ª£i: {e}")
    finally:
        crawler.close()


if __name__ == "__main__":
    main()
