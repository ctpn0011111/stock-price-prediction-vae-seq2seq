import requests
from bs4 import BeautifulSoup
import json
import time
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import uuid


class FPTNewsCrawler:
    def __init__(self):
        self.base_url = "https://fpt.com/vi/tin-tuc/page/"
        self.dataset_path = "../dataset/link/fpt/"
        self.setup_driver()
        self.ensure_directory_exists()

    def setup_driver(self):
        """Thiáº¿t láº­p Chrome driver vá»›i cÃ¡c options tá»‘i Æ°u cho Ubuntu"""
        chrome_options = Options()

        # Ubuntu optimized options
        chrome_options.add_argument("--headless")  # Cháº¡y áº©n browser
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-plugins")
        chrome_options.add_argument("--disable-images")  # TÄƒng tá»‘c load
        chrome_options.add_argument("--disable-javascript-harmony")
        chrome_options.add_argument("--window-size=1920,1080")

        # Ubuntu Chrome User-Agent
        chrome_options.add_argument(
            "--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )

        # Performance optimization
        chrome_options.add_argument("--memory-pressure-off")
        chrome_options.add_argument("--max_old_space_size=4096")

        self.driver = webdriver.Chrome(options=chrome_options)
        self.driver.implicitly_wait(5)  # Implicit wait ngáº¯n

    def ensure_directory_exists(self):
        """Táº¡o thÆ° má»¥c lÆ°u trá»¯ náº¿u chÆ°a tá»“n táº¡i"""
        if not os.path.exists(self.dataset_path):
            os.makedirs(self.dataset_path)

    def wait_for_content_load(self, timeout=15):
        """Äá»£i ná»™i dung load hoÃ n toÃ n vá»›i nhiá»u chiáº¿n lÆ°á»£c"""
        wait = WebDriverWait(self.driver, timeout)

        try:
            print("Äang Ä‘á»£i ná»™i dung load...")

            # Strategy 1: Äá»£i elements news-card-meta xuáº¥t hiá»‡n
            wait.until(
                EC.presence_of_element_located((By.CLASS_NAME, "news-card-meta"))
            )
            print("âœ“ TÃ¬m tháº¥y elements news-card-meta")

            # Strategy 2: Äá»£i thÃªm Ä‘á»ƒ Ä‘áº£m báº£o táº¥t cáº£ elements Ä‘Ã£ load
            time.sleep(2)

            # Strategy 3: Kiá»ƒm tra sá»‘ lÆ°á»£ng elements cÃ³ tÄƒng khÃ´ng
            initial_count = len(
                self.driver.find_elements(By.CLASS_NAME, "news-card-meta")
            )
            print(f"Sá»‘ bÃ i viáº¿t ban Ä‘áº§u: {initial_count}")

            # Äá»£i thÃªm 5s Ä‘á»ƒ xem cÃ³ load thÃªm khÃ´ng
            time.sleep(5)
            final_count = len(
                self.driver.find_elements(By.CLASS_NAME, "news-card-meta")
            )
            print(f"Sá»‘ bÃ i viáº¿t sau 5s: {final_count}")

            # Náº¿u váº«n cÃ²n load thÃ¬ Ä‘á»£i thÃªm
            if final_count > initial_count:
                print("Váº«n Ä‘ang load thÃªm ná»™i dung, Ä‘á»£i thÃªm 5s...")
                time.sleep(5)

            return True

        except TimeoutException:
            print(
                f"Timeout sau {timeout}s - CÃ³ thá»ƒ trang khÃ´ng cÃ³ ná»™i dung hoáº·c load cháº­m"
            )
            return False

    def crawl_page(self, page_number):
        """Crawl má»™t trang cá»¥ thá»ƒ vá»›i smart waiting"""
        url = f"{self.base_url}{page_number}"
        print(f"\nğŸ”„ Äang crawl trang {page_number}: {url}")

        try:
            # Load trang web
            start_time = time.time()
            self.driver.get(url)

            # Äá»£i ná»™i dung load vá»›i smart strategy
            if not self.wait_for_content_load(timeout=20):
                print(f"âŒ KhÃ´ng thá»ƒ load ná»™i dung trang {page_number}")
                return []

            load_time = time.time() - start_time
            print(f"â±ï¸ Thá»i gian load: {load_time:.1f}s")

            # Láº¥y HTML sau khi Ä‘Ã£ load xong
            html_content = self.driver.page_source
            soup = BeautifulSoup(html_content, "html.parser")

            # TÃ¬m cÃ¡c tháº» div cÃ³ class 'news-card-meta'
            news_cards = soup.find_all("div", class_="news-card-meta")

            if not news_cards:
                print(
                    f"âŒ KhÃ´ng tÃ¬m tháº¥y elements news-card-meta trÃªn trang {page_number}"
                )
                # Debug: LÆ°u HTML Ä‘á»ƒ kiá»ƒm tra
                with open(f"debug_page_{page_number}.html", "w", encoding="utf-8") as f:
                    f.write(html_content)
                print(f"ğŸ” ÄÃ£ lÆ°u HTML debug vÃ o debug_page_{page_number}.html")
                return []

            page_data = []
            for i, card in enumerate(news_cards, 1):
                try:
                    # TÃ¬m tháº» a con cá»§a div
                    link_tag = card.find("a")
                    if link_tag:
                        # Láº¥y href
                        href = link_tag.get("href", "")

                        # Láº¥y tháº» h2 con cá»§a a
                        h2_tag = link_tag.find("h2")
                        title = h2_tag.get_text(strip=True) if h2_tag else ""

                        if href and title:
                            # Táº¡o URL Ä‘áº§y Ä‘á»§ náº¿u href lÃ  relative
                            if href.startswith("/"):
                                full_url = f"https://fpt.com{href}"
                            elif href.startswith("http"):
                                full_url = href
                            else:
                                full_url = f"https://fpt.com/vi/{href}"

                            news_item = {
                                "id": str(uuid.uuid4()),
                                "title": title,
                                "url": full_url,
                                "ngay_crawl": datetime.now().strftime(
                                    "%Y-%m-%d %H:%M:%S"
                                ),
                            }
                            page_data.append(news_item)
                            print(f"  ğŸ“° {i}. {title[:60]}...")

                except Exception as e:
                    print(f"âš ï¸ Lá»—i khi xá»­ lÃ½ bÃ i viáº¿t {i}: {str(e)}")
                    continue

            print(f"âœ… Trang {page_number}: TÃ¬m tháº¥y {len(page_data)} bÃ i viáº¿t")
            return page_data

        except Exception as e:
            print(f"âŒ Lá»—i khi crawl trang {page_number}: {str(e)}")
            return []

    def save_page_data(self, page_number, data):
        """LÆ°u dá»¯ liá»‡u cá»§a má»™t trang vÃ o file JSON"""
        if not data:
            print(f"âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ lÆ°u cho trang {page_number}")
            return

        filename = f"page_{page_number}.json"
        filepath = os.path.join(self.dataset_path, filename)

        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ÄÃ£ lÆ°u dá»¯ liá»‡u trang {page_number} vÃ o {filepath}")
        except Exception as e:
            print(f"âŒ Lá»—i khi lÆ°u file trang {page_number}: {str(e)}")

    def crawl_multiple_pages(self, start_page, end_page):
        """Crawl nhiá»u trang tá»« start_page Ä‘áº¿n end_page"""
        print(f"ğŸš€ Báº¯t Ä‘áº§u crawl tá»« trang {start_page} Ä‘áº¿n trang {end_page}")
        print(f"ğŸ“ Dá»¯ liá»‡u sáº½ Ä‘Æ°á»£c lÆ°u vÃ o: {os.path.abspath(self.dataset_path)}")

        all_data = []
        successful_pages = 0

        for page_num in range(start_page, end_page + 1):
            print(f"\n{'='*50}")
            print(f"ğŸ“„ TRANG {page_num}/{end_page}")
            print(f"{'='*50}")

            page_data = self.crawl_page(page_num)

            if page_data:
                # LÆ°u dá»¯ liá»‡u trang ngay sau khi crawl xong
                self.save_page_data(page_num, page_data)
                all_data.extend(page_data)
                successful_pages += 1

            # Nghá»‰ giá»¯a cÃ¡c trang Ä‘á»ƒ trÃ¡nh bá»‹ block
            if page_num < end_page:
                print("ğŸ˜´ Nghá»‰ 3 giÃ¢y trÆ°á»›c khi crawl trang tiáº¿p theo...")
                time.sleep(3)

        # LÆ°u tá»•ng há»£p táº¥t cáº£ dá»¯ liá»‡u
        if all_data:
            summary_file = f"all_pages_{start_page}_to_{end_page}.json"
            summary_path = os.path.join(self.dataset_path, summary_file)

            try:
                with open(summary_path, "w", encoding="utf-8") as f:
                    json.dump(all_data, f, ensure_ascii=False, indent=2)
                print(
                    f"\nğŸ‰ ÄÃ£ lÆ°u tá»•ng há»£p {len(all_data)} bÃ i viáº¿t vÃ o {summary_path}"
                )
            except Exception as e:
                print(f"âŒ Lá»—i khi lÆ°u file tá»•ng há»£p: {str(e)}")

        print(f"\n{'='*50}")
        print(f"ğŸ“Š Káº¾T QUáº¢ CUá»I CÃ™NG")
        print(f"{'='*50}")
        print(f"âœ… Trang thÃ nh cÃ´ng: {successful_pages}/{end_page - start_page + 1}")
        print(f"ğŸ“° Tá»•ng bÃ i viáº¿t: {len(all_data)}")
        print(f"ğŸ“ ThÆ° má»¥c lÆ°u trá»¯: {os.path.abspath(self.dataset_path)}")

        return all_data

    def test_single_page(self, page_number=1):
        """Test crawl má»™t trang Ä‘á»ƒ kiá»ƒm tra"""
        print(f"ğŸ§ª Test crawl trang {page_number}...")
        data = self.crawl_page(page_number)

        if data:
            print(f"\nğŸ“‹ Preview {min(3, len(data))} bÃ i viáº¿t Ä‘áº§u:")
            for i, item in enumerate(data[:3], 1):
                print(f"  {i}. {item['title']}")
                print(f"     ğŸ”— {item['url']}")
                print(f"     ğŸ• {item['ngay_crawl']}")
                print()

        return data

    def close(self):
        """ÄÃ³ng driver"""
        if hasattr(self, "driver"):
            self.driver.quit()
            print("ğŸ”’ ÄÃ£ Ä‘Ã³ng browser")


def main():
    """HÃ m main Ä‘á»ƒ cháº¡y crawler"""
    crawler = FPTNewsCrawler()

    try:
        print("ğŸ¯ FPT NEWS CRAWLER")
        print("=" * 30)
        print("1. Test crawl trang 1")
        print("2. Crawl nhiá»u trang")
        choice = input("Chá»n (1/2): ").strip()

        if choice == "1":
            result = crawler.test_single_page(1)
            if result:
                save_choice = input("\nLÆ°u káº¿t quáº£ test? (y/n): ").strip().lower()
                if save_choice == "y":
                    crawler.save_page_data(1, result)
        else:
            # Nháº­p trang báº¯t Ä‘áº§u vÃ  trang káº¿t thÃºc
            start_page = int(input("Nháº­p trang báº¯t Ä‘áº§u: "))
            end_page = int(input("Nháº­p trang káº¿t thÃºc: "))

            if start_page > end_page:
                print("âŒ Trang báº¯t Ä‘áº§u pháº£i nhá» hÆ¡n hoáº·c báº±ng trang káº¿t thÃºc!")
                return

            # XÃ¡c nháº­n
            total_pages = end_page - start_page + 1
            estimated_time = total_pages * 15  # ~15s per page
            print(f"\nğŸ“Š Sáº½ crawl {total_pages} trang")
            print(f"â±ï¸ Thá»i gian Æ°á»›c tÃ­nh: ~{estimated_time//60}p{estimated_time%60}s")

            confirm = input("Tiáº¿p tá»¥c? (y/n): ").strip().lower()
            if confirm != "y":
                print("ğŸš« ÄÃ£ há»§y")
                return

            # Báº¯t Ä‘áº§u crawl
            start_time = time.time()
            result = crawler.crawl_multiple_pages(start_page, end_page)
            end_time = time.time()

            print(
                f"\nâ±ï¸ Tá»•ng thá»i gian: {(end_time - start_time)//60:.0f}p{(end_time - start_time)%60:.0f}s"
            )

    except KeyboardInterrupt:
        print("\nğŸ›‘ ÄÃ£ dá»«ng crawl theo yÃªu cáº§u ngÆ°á»i dÃ¹ng")
    except ValueError:
        print("âŒ Vui lÃ²ng nháº­p sá»‘ há»£p lá»‡!")
    except Exception as e:
        print(f"âŒ Lá»—i: {str(e)}")
    finally:
        crawler.close()


if __name__ == "__main__":
    main()
