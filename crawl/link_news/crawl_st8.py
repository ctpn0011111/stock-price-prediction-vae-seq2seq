import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
import json
import os
from datetime import datetime
import time
import logging
import random


class ST8Crawler:
    def __init__(self):
        self.base_url = "https://st8.vn/truyen-thong/tin-tuc-su-kien"
        self.domain = "https://st8.vn"
        self.output_dir = "../dataset/link/st8"
        self.setup_logging()
        self.driver = None
        self.create_output_directory()
        self.max_retries = 3
        self.restart_interval = 20  # Restart driver every 20 pages

    def setup_logging(self):
        """Thi·∫øt l·∫≠p logging ƒë·ªÉ theo d√µi qu√° tr√¨nh crawl"""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[logging.FileHandler("st8_crawler.log"), logging.StreamHandler()],
        )
        self.logger = logging.getLogger(__name__)

    def setup_driver(self):
        """Thi·∫øt l·∫≠p Selenium WebDriver v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u"""
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-plugins")
        chrome_options.add_argument("--disable-images")
        chrome_options.add_argument("--memory-pressure-off")
        chrome_options.add_argument("--max_old_space_size=4096")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument(
            "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )

        # Th√™m prefs ƒë·ªÉ t·ªëi ∆∞u h√≥a
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.default_content_settings.popups": 0,
            "profile.managed_default_content_settings.media_stream": 2,
        }
        chrome_options.add_experimental_option("prefs", prefs)

        try:
            if self.driver:
                self.driver.quit()

            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.set_page_load_timeout(30)
            self.logger.info("WebDriver ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng")
            return True
        except Exception as e:
            self.logger.error(f"L·ªói kh·ªüi t·∫°o WebDriver: {e}")
            return False

    def restart_driver(self):
        """Kh·ªüi ƒë·ªông l·∫°i WebDriver ƒë·ªÉ tr√°nh memory leak"""
        self.logger.info("ƒêang kh·ªüi ƒë·ªông l·∫°i WebDriver...")
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass
        return self.setup_driver()

    def create_output_directory(self):
        """T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ n·∫øu ch∆∞a t·ªìn t·∫°i"""
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
            self.logger.info(f"ƒê√£ t·∫°o th∆∞ m·ª•c: {self.output_dir}")

    def get_page_url(self, page_num):
        """T·∫°o URL cho trang c·ª• th·ªÉ"""
        if page_num == 1:
            return self.base_url
        else:
            return f"{self.base_url}?page={page_num}"

    def safe_get_page(self, url, retries=0):
        """Truy c·∫≠p trang web m·ªôt c√°ch an to√†n v·ªõi retry logic"""
        if retries >= self.max_retries:
            self.logger.error(
                f"ƒê√£ th·ª≠ {self.max_retries} l·∫ßn, kh√¥ng th·ªÉ truy c·∫≠p {url}"
            )
            return False

        try:
            if not self.driver:
                if not self.setup_driver():
                    return False

            self.driver.get(url)

            # ƒê·ª£i trang load
            WebDriverWait(self.driver, 25).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            # ƒê·ª£i th√™m m·ªôt ch√∫t ƒë·ªÉ ƒë·∫£m b·∫£o content ƒë∆∞·ª£c load
            time.sleep(3)
            return True

        except TimeoutException:
            self.logger.warning(
                f"Timeout khi t·∫£i trang {url}, th·ª≠ l·∫°i... (l·∫ßn {retries + 1})"
            )
            time.sleep(random.uniform(3, 7))
            return self.safe_get_page(url, retries + 1)

        except WebDriverException as e:
            self.logger.error(f"L·ªói WebDriver: {e}")
            if "chrome not reachable" in str(e).lower() or "session" in str(e).lower():
                self.logger.info("Kh·ªüi ƒë·ªông l·∫°i WebDriver do l·ªói session...")
                if self.restart_driver():
                    return self.safe_get_page(url, retries + 1)
            return False

        except Exception as e:
            self.logger.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")
            time.sleep(random.uniform(2, 5))
            return self.safe_get_page(url, retries + 1)

    def check_page_exists(self, page_num):
        """Ki·ªÉm tra xem trang c√≥ t·ªìn t·∫°i v√† c√≥ d·ªØ li·ªáu hay kh√¥ng"""
        url = self.get_page_url(page_num)

        if not self.safe_get_page(url):
            return False

        try:
            # Ki·ªÉm tra div c√≥ class col-md-8 ho·∫∑c col-md-4
            divs = self.driver.find_elements(
                By.CSS_SELECTOR,
                "div.col-md-8.wow.fadeInUpZ, div.col-md-4.wow.fadeInUpZ",
            )

            if divs and len(divs) > 0:
                self.logger.info(f"Trang {page_num} c√≥ {len(divs)} div ch·ª©a b√†i vi·∫øt")
                return True
            else:
                self.logger.warning(
                    f"Trang {page_num} kh√¥ng c√≥ b√†i vi·∫øt ho·∫∑c ƒë√£ h·∫øt d·ªØ li·ªáu"
                )
                return False

        except Exception as e:
            self.logger.error(f"L·ªói khi ki·ªÉm tra trang {page_num}: {e}")
            return False

    def extract_data_from_page(self, page_num):
        """Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ m·ªôt trang"""
        url = self.get_page_url(page_num)
        articles = []

        if not self.safe_get_page(url):
            return articles

        try:
            time.sleep(4)
            soup = BeautifulSoup(self.driver.page_source, "html.parser")

            # L·∫•y c·∫£ md-8 v√† md-4
            content_divs = soup.select(
                "div.col-md-8.wow.fadeInUpZ, div.col-md-4.wow.fadeInUpZ"
            )

            self.logger.info(
                f"Trang {page_num}: T√¨m th·∫•y {len(content_divs)} div ch·ª©a b√†i vi·∫øt"
            )

            processed_urls = set()

            for div in content_divs:
                try:
                    h2_title = div.find("h2", class_="post__title")
                    if not h2_title:
                        continue

                    a_tag = h2_title.find("a", class_="smooth")
                    if not a_tag:
                        continue

                    href = a_tag.get("href", "").strip()
                    title = a_tag.get_text(strip=True)

                    if not href or not title:
                        continue

                    # X·ª≠ l√Ω URL
                    if href.startswith("/"):
                        full_url = f"{self.domain}{href}"
                    elif href.startswith("http"):
                        full_url = href
                    else:
                        full_url = f"{self.domain}/{href}"

                    if full_url in processed_urls:
                        continue
                    processed_urls.add(full_url)

                    if len(title) < 5:
                        continue

                    article = {"title": title, "url": full_url}
                    articles.append(article)

                except Exception as e:
                    self.logger.warning(
                        f"L·ªói khi x·ª≠ l√Ω m·ªôt b√†i vi·∫øt tr√™n trang {page_num}: {e}"
                    )
                    continue

            self.logger.info(
                f"Trang {page_num}: ƒê√£ tr√≠ch xu·∫•t {len(articles)} b√†i vi·∫øt unique"
            )
            return articles

        except Exception as e:
            self.logger.error(f"L·ªói khi tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ trang {page_num}: {e}")
            return []

    def save_page_data(self, articles, page_num):
        """L∆∞u d·ªØ li·ªáu c·ªßa m·ªôt trang v√†o file JSON"""
        if not articles:
            self.logger.warning(f"Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u cho trang {page_num}")
            return

        filename = f"st8_page_{page_num}.json"
        filepath = os.path.join(self.output_dir, filename)

        # Th√™m ID t·ª± ƒë·ªông tƒÉng v√† ng√†y crawl
        crawl_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        formatted_articles = []
        for idx, article in enumerate(articles, 1):
            formatted_article = {
                "id": f"page_{page_num}_{idx}",
                "title": article["title"],
                "url": article["url"],
                "crawl_date": crawl_date,
            }
            formatted_articles.append(formatted_article)

        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(formatted_articles, f, ensure_ascii=False, indent=2)

            self.logger.info(
                f"ƒê√£ l∆∞u {len(formatted_articles)} b√†i vi·∫øt v√†o {filepath}"
            )

        except Exception as e:
            self.logger.error(f"L·ªói khi l∆∞u file {filepath}: {e}")

    def save_all_data(self, all_articles):
        """L∆∞u t·∫•t c·∫£ d·ªØ li·ªáu v√†o m·ªôt file t·ªïng h·ª£p"""
        if not all_articles:
            return

        filename = "st8_all_articles.json"
        filepath = os.path.join(self.output_dir, filename)

        # Th√™m ID t·ª± ƒë·ªông tƒÉng v√† ng√†y crawl
        crawl_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        formatted_articles = []
        for idx, article in enumerate(all_articles, 1):
            formatted_article = {
                "id": idx,
                "title": article["title"],
                "url": article["url"],
                "crawl_date": crawl_date,
            }
            formatted_articles.append(formatted_article)

        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(formatted_articles, f, ensure_ascii=False, indent=2)

            self.logger.info(
                f"ƒê√£ l∆∞u t·ªïng c·ªông {len(formatted_articles)} b√†i vi·∫øt v√†o {filepath}"
            )

            # L∆∞u th√™m file backup v·ªõi timestamp
            backup_filename = (
                f"st8_all_articles_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
            backup_filepath = os.path.join(self.output_dir, backup_filename)

            with open(backup_filepath, "w", encoding="utf-8") as f:
                json.dump(formatted_articles, f, ensure_ascii=False, indent=2)

            self.logger.info(f"ƒê√£ t·∫°o file backup: {backup_filename}")

        except Exception as e:
            self.logger.error(f"L·ªói khi l∆∞u file t·ªïng h·ª£p: {e}")

    def crawl_pages_range(self, start_page, end_page):
        """Crawl c√°c trang trong kho·∫£ng t·ª´ start_page ƒë·∫øn end_page"""
        all_articles = []

        # Ki·ªÉm tra input h·ª£p l·ªá
        if start_page < 1 or end_page < start_page:
            self.logger.error(
                "S·ªë trang kh√¥ng h·ª£p l·ªá. start_page ph·∫£i >= 1 v√† end_page >= start_page"
            )
            return

        self.logger.info(
            f"B·∫Øt ƒë·∫ßu crawl d·ªØ li·ªáu t·ª´ trang {start_page} ƒë·∫øn trang {end_page}"
        )

        # Kh·ªüi t·∫°o driver
        if not self.setup_driver():
            self.logger.error("Kh√¥ng th·ªÉ kh·ªüi t·∫°o WebDriver")
            return

        for page_num in range(start_page, end_page + 1):
            self.logger.info(f"ƒêang crawl trang {page_num}...")

            # Kh·ªüi ƒë·ªông l·∫°i driver ƒë·ªãnh k·ª≥
            if (
                page_num - start_page
            ) % self.restart_interval == 0 and page_num != start_page:
                self.logger.info(
                    f"Kh·ªüi ƒë·ªông l·∫°i driver sau {self.restart_interval} trang..."
                )
                if not self.restart_driver():
                    self.logger.error("Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông l·∫°i driver")
                    break

            # Ki·ªÉm tra xem trang c√≥ t·ªìn t·∫°i kh√¥ng
            if not self.check_page_exists(page_num):
                self.logger.warning(
                    f"Trang {page_num} kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng c√≥ d·ªØ li·ªáu, b·ªè qua"
                )
                continue

            # Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ trang
            articles = self.extract_data_from_page(page_num)

            if articles:
                # L∆∞u d·ªØ li·ªáu trang hi·ªán t·∫°i
                self.save_page_data(articles, page_num)

                # Th√™m v√†o danh s√°ch t·ªïng
                all_articles.extend(articles)

                self.logger.info(
                    f"ƒê√£ ho√†n th√†nh trang {page_num} v·ªõi {len(articles)} b√†i vi·∫øt"
                )
            else:
                self.logger.warning(f"Trang {page_num} kh√¥ng c√≥ d·ªØ li·ªáu")

            # Random delay ƒë·ªÉ tr√°nh b·ªã block
            time.sleep(random.uniform(3, 6))

        # L∆∞u file t·ªïng h·ª£p
        if all_articles:
            self.save_all_data(all_articles)
            self.logger.info(
                f"Ho√†n th√†nh crawl! T·ªïng c·ªông {len(all_articles)} b√†i vi·∫øt t·ª´ {end_page - start_page + 1} trang"
            )
        else:
            self.logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c crawl")

    def crawl_all_pages(self):
        """Crawl t·∫•t c·∫£ c√°c trang t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi"""
        page_num = 1
        all_articles = []
        consecutive_failures = 0
        max_consecutive_failures = 5

        self.logger.info("B·∫Øt ƒë·∫ßu crawl d·ªØ li·ªáu t·ª´ ST8")

        # Kh·ªüi t·∫°o driver
        if not self.setup_driver():
            self.logger.error("Kh√¥ng th·ªÉ kh·ªüi t·∫°o WebDriver")
            return

        while True:
            self.logger.info(f"ƒêang crawl trang {page_num}...")

            # Kh·ªüi ƒë·ªông l·∫°i driver m·ªói X trang ƒë·ªÉ tr√°nh memory leak
            if page_num % self.restart_interval == 0:
                self.logger.info(
                    f"Kh·ªüi ƒë·ªông l·∫°i driver sau {self.restart_interval} trang..."
                )
                if not self.restart_driver():
                    self.logger.error("Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông l·∫°i driver")
                    break

            # Ki·ªÉm tra xem trang c√≥ t·ªìn t·∫°i kh√¥ng
            if not self.check_page_exists(page_num):
                consecutive_failures += 1
                if consecutive_failures >= max_consecutive_failures:
                    self.logger.info(
                        f"ƒê√£ crawl xong. {consecutive_failures} trang li√™n ti·∫øp kh√¥ng c√≥ d·ªØ li·ªáu."
                    )
                    break
                else:
                    self.logger.warning(
                        f"Trang {page_num} kh√¥ng c√≥ d·ªØ li·ªáu, th·ª≠ trang ti·∫øp theo..."
                    )
                    page_num += 1
                    continue

            # Reset counter khi t√¨m th·∫•y trang c√≥ d·ªØ li·ªáu
            consecutive_failures = 0

            # Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ trang
            articles = self.extract_data_from_page(page_num)

            if articles:
                # L∆∞u d·ªØ li·ªáu trang hi·ªán t·∫°i
                self.save_page_data(articles, page_num)

                # Th√™m v√†o danh s√°ch t·ªïng
                all_articles.extend(articles)

                self.logger.info(
                    f"ƒê√£ ho√†n th√†nh trang {page_num} v·ªõi {len(articles)} b√†i vi·∫øt"
                )
            else:
                self.logger.warning(f"Trang {page_num} kh√¥ng c√≥ d·ªØ li·ªáu")

            # TƒÉng s·ªë trang
            page_num += 1

            # Random delay ƒë·ªÉ tr√°nh b·ªã block
            time.sleep(random.uniform(3, 6))

        # L∆∞u file t·ªïng h·ª£p
        if all_articles:
            self.save_all_data(all_articles)
            self.logger.info(
                f"Ho√†n th√†nh crawl! T·ªïng c·ªông {len(all_articles)} b√†i vi·∫øt t·ª´ {page_num-1} trang"
            )
        else:
            self.logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c crawl")

    def crawl_single_page(self, page_num=1):
        """Crawl m·ªôt trang duy nh·∫•t"""
        self.logger.info(f"B·∫Øt ƒë·∫ßu crawl trang {page_num}")

        # Kh·ªüi t·∫°o driver
        if not self.setup_driver():
            self.logger.error("Kh√¥ng th·ªÉ kh·ªüi t·∫°o WebDriver")
            return

        try:
            # Ki·ªÉm tra trang c√≥ t·ªìn t·∫°i kh√¥ng
            if not self.check_page_exists(page_num):
                self.logger.error(
                    f"Trang {page_num} kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng c√≥ d·ªØ li·ªáu"
                )
                return

            # Tr√≠ch xu·∫•t d·ªØ li·ªáu
            articles = self.extract_data_from_page(page_num)

            if articles:
                # L∆∞u d·ªØ li·ªáu
                self.save_page_data(articles, page_num)
                self.logger.info(
                    f"Ho√†n th√†nh crawl trang {page_num}! T·ªïng c·ªông {len(articles)} b√†i vi·∫øt"
                )
            else:
                self.logger.warning(f"Trang {page_num} kh√¥ng c√≥ d·ªØ li·ªáu")

        except Exception as e:
            self.logger.error(f"L·ªói trong qu√° tr√¨nh crawl trang {page_num}: {e}")
        finally:
            self.close()

    def close(self):
        """ƒê√≥ng WebDriver"""
        if self.driver:
            try:
                self.driver.quit()
                self.logger.info("ƒê√£ ƒë√≥ng WebDriver")
            except:
                pass


def get_crawl_mode():
    """L·∫•y input t·ª´ ng∆∞·ªùi d√πng ƒë·ªÉ ch·ªçn ch·∫ø ƒë·ªô crawl"""
    print("=== ST8 Website Crawler ===")
    print("Ch·ªçn ch·∫ø ƒë·ªô crawl:")
    print("1. Crawl m·ªôt trang duy nh·∫•t")
    print("2. Crawl theo kho·∫£ng trang (ch·ªçn trang b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c)")
    print("3. Crawl t·∫•t c·∫£ trang (t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi)")

    while True:
        try:
            choice = int(input("\nNh·∫≠p l·ª±a ch·ªçn (1, 2 ho·∫∑c 3): "))
            if choice in [1, 2, 3]:
                return choice
            else:
                print("Vui l√≤ng nh·∫≠p 1, 2 ho·∫∑c 3")
        except ValueError:
            print("Vui l√≤ng nh·∫≠p s·ªë h·ª£p l·ªá")


def get_single_page():
    """L·∫•y s·ªë trang duy nh·∫•t t·ª´ ng∆∞·ªùi d√πng"""
    while True:
        try:
            page_num = int(input("Nh·∫≠p s·ªë trang mu·ªën crawl (m·∫∑c ƒë·ªãnh l√† 1): ") or "1")
            if page_num >= 1:
                return page_num
            else:
                print("S·ªë trang ph·∫£i >= 1")
        except ValueError:
            print("Vui l√≤ng nh·∫≠p s·ªë h·ª£p l·ªá")


def get_page_range():
    """L·∫•y kho·∫£ng trang t·ª´ ng∆∞·ªùi d√πng"""
    while True:
        try:
            start_page = int(input("Nh·∫≠p trang b·∫Øt ƒë·∫ßu: "))
            if start_page < 1:
                print("Trang b·∫Øt ƒë·∫ßu ph·∫£i >= 1")
                continue

            end_page = int(input("Nh·∫≠p trang k·∫øt th√∫c: "))
            if end_page < start_page:
                print("Trang k·∫øt th√∫c ph·∫£i >= trang b·∫Øt ƒë·∫ßu")
                continue

            return start_page, end_page
        except ValueError:
            print("Vui l√≤ng nh·∫≠p s·ªë h·ª£p l·ªá")


def main():
    crawler = ST8Crawler()

    try:
        choice = get_crawl_mode()

        if choice == 1:
            page_num = get_single_page()
            print(f"\nüöÄ B·∫Øt ƒë·∫ßu crawl trang {page_num}...")
            crawler.crawl_single_page(page_num)
        elif choice == 2:
            start_page, end_page = get_page_range()
            print(f"\nüöÄ B·∫Øt ƒë·∫ßu crawl t·ª´ trang {start_page} ƒë·∫øn trang {end_page}...")
            crawler.crawl_pages_range(start_page, end_page)
        else:
            print("\nüöÄ B·∫Øt ƒë·∫ßu crawl t·∫•t c·∫£ trang...")
            crawler.crawl_all_pages()

    except KeyboardInterrupt:
        crawler.logger.info("Crawl b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
    except Exception as e:
        crawler.logger.error(f"L·ªói kh√¥ng mong ƒë·ª£i: {e}")
    finally:
        if hasattr(crawler, "driver") and crawler.driver:
            crawler.close()


if __name__ == "__main__":
    main()
