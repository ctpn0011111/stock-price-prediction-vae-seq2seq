import os
import json
import time
from datetime import datetime
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    TimeoutException,
    ElementClickInterceptedException,
    NoSuchElementException,
)

# T·∫°o th∆∞ m·ª•c l∆∞u file
output_dir = "../dataset/link/pvg/"
os.makedirs(output_dir, exist_ok=True)

URL = "https://www.genco3.com/tin-tuc/tin-nganh-dien.html"


def crawl_genco3(max_clicks=50):
    # L·∫•y chromedriver path t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
    chromedriver_path = os.getenv("CHROMEDRIVER_PATH")
    if not chromedriver_path:
        raise EnvironmentError(
            "‚ö†Ô∏è Bi·∫øn m√¥i tr∆∞·ªùng CHROMEDRIVER_PATH ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p"
        )

    # C·∫•u h√¨nh Chrome
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")  # b·ªè n·∫øu mu·ªën nh√¨n tr√¨nh duy·ªát
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    service = Service(chromedriver_path)
    driver = webdriver.Chrome(service=service, options=options)

    driver.get(URL)
    wait = WebDriverWait(driver, 10)

    current_id = 0
    all_data = []
    seen_urls = set()

    for click_num in range(1, max_clicks + 1):
        try:
            # Ch·ªù n√∫t "Xem th√™m" hi·ªÉn th·ªã
            btn = wait.until(EC.element_to_be_clickable((By.ID, "btnLoadMore")))
            driver.execute_script("arguments[0].click();", btn)  # click b·∫±ng JS
            time.sleep(2)  # ch·ªù load
        except (
            TimeoutException,
            ElementClickInterceptedException,
            NoSuchElementException,
        ):
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y ho·∫∑c kh√¥ng click ƒë∆∞·ª£c n√∫t Xem th√™m.")
            break

        # Parse HTML sau khi click
        soup = BeautifulSoup(driver.page_source, "html.parser")
        posts = soup.find_all("div", class_="nw-card-item")

        for post in posts:
            card_content = post.find("div", class_="card-content")
            if card_content:
                p_tag = card_content.find("p")
                a_tag = p_tag.find("a") if p_tag else None
                if a_tag and a_tag.get("href"):
                    url = "https://www.genco3.com" + a_tag["href"]
                    if url not in seen_urls:  # tr√°nh tr√πng l·∫∑p
                        seen_urls.add(url)
                        current_id += 1
                        all_data.append(
                            {
                                "id": current_id,
                                "title": a_tag.get_text(strip=True),
                                "url": url,
                                "ngay_crawl": datetime.now().strftime(
                                    "%Y-%m-%d %H:%M:%S"
                                ),
                            }
                        )

        print(f"üëâ ƒê√£ crawl xong l·∫ßn click {click_num}, t·ªïng {len(all_data)} b√†i vi·∫øt.")

    driver.quit()

    # Sau khi crawl xong m·ªõi l∆∞u 1 file duy nh·∫•t
    output_file = os.path.join(output_dir, "genco3_data.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=4)

    print(f"‚úÖ ƒê√£ l∆∞u to√†n b·ªô d·ªØ li·ªáu v√†o: {output_file}")


if __name__ == "__main__":
    crawl_genco3(max_clicks=100)
